{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T22:38:22.888368Z",
     "start_time": "2025-10-22T22:38:22.842577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "69af0124c2583da6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-22T22:38:25.483094Z",
     "start_time": "2025-10-22T22:38:22.897374Z"
    }
   },
   "source": [
    "# Imports for utilities\n",
    "import os\n",
    "from utils.config import Config  # Pydantic data model for config\n",
    "from utils.logging import init_logging\n",
    "from utils.seed import set_seed\n",
    "\n",
    "# Define configuration as a nested dictionary (or use JSON format)\n",
    "config_dict = {\n",
    "    \"system\": {\n",
    "        \"numexpr_threads\": 4,\n",
    "        \"deterministic\": False,\n",
    "        \"cpu\": {\n",
    "          \"intra_op_threads\": None,\n",
    "          \"inter_op_threads\": None,\n",
    "          \"omp_num_threads\": None,\n",
    "          \"mkl_num_threads\": None,\n",
    "          \"kmp_affinity\": \"granularity=fine,compact,1,0\",\n",
    "          \"start_method\": \"forkserver\",\n",
    "          \"pin_affinity_cores\": None\n",
    "        },\n",
    "        \"dataloader\": {\n",
    "          \"num_workers\": os.cpu_count()//2,           # auto -> max(2, cores-1)\n",
    "          \"prefetch_factor\": 4,\n",
    "          \"persistent_workers\": True,\n",
    "          \"pin_memory\": True,\n",
    "          \"pin_memory_device\": \"cuda\",\n",
    "          \"non_blocking\": True\n",
    "        },\n",
    "        \"cuda\": {\n",
    "          \"enabled\": True,\n",
    "          \"device_ids\": None,            # auto -> all visible\n",
    "          \"allow_tf32\": True,\n",
    "          \"matmul_precision\": \"high\",\n",
    "          \"cudnn_benchmark\": True,\n",
    "          \"cudnn_deterministic\": None,\n",
    "          \"amp\": True,\n",
    "          \"amp_dtype\": \"bf16\",           # prefer bf16 on Ampere+ if available\n",
    "          \"grad_scaler_enabled\": True,\n",
    "          \"compile_mode\": \"reduce-overhead\",\n",
    "          \"compile_fullgraph\": False,\n",
    "          \"uva\": False\n",
    "        },\n",
    "        \"ddp\": {\n",
    "          \"enabled\": False,\n",
    "          \"backend\": \"nccl\",\n",
    "          \"find_unused_parameters\": False,\n",
    "          \"gradient_as_bucket_view\": True,\n",
    "          \"broadcast_buffers\": False,\n",
    "          \"static_graph\": False\n",
    "        }\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"csv_path\": \"raw/diabetic_data.csv\",            # raw data file\n",
    "        \"ids_mapping_path\": \"raw/IDS_mapping.csv\",          # ID mapping file for codes\n",
    "        \"target\": {\"name\": \"readmitted\", \"positive_values\": [\"<30\"], \"binarized_name\": \"readmitted_binary\"},  # predict 30-day readmit\n",
    "        \"identifier_cols\": {\"encounter_id\": \"encounter_id\", \"patient_id\": \"patient_nbr\"},\n",
    "        \"filters\": {\n",
    "            \"exclude_discharge_to_ids\": [11, 13, 14, 19, 20, 21],\n",
    "            \"first_encounter_per_patient\": True,\n",
    "        },\n",
    "        \"columns\": {\n",
    "            # Numeric features (counts, etc.)\n",
    "            \"numeric\": [\"time_in_hospital\", \"num_lab_procedures\", \"num_procedures\",\n",
    "                        \"num_medications\", \"number_outpatient\", \"number_emergency\",\n",
    "                        \"number_inpatient\", \"number_diagnoses\"],\n",
    "            # Low-cardinality categoricals (will be one-hot or label encoded as features)\n",
    "            \"categorical_low_card\": [\"race\", \"gender\", \"age\", \"max_glu_serum\", \"A1Cresult\", \"change\", \"diabetesMed\"],\n",
    "            # High-cardinality categorical columns to be turned into separate nodes\n",
    "            \"icd_cols\": [\"diag_1\", \"diag_2\", \"diag_3\"],          # diagnosis code columns\n",
    "            \"drug_cols\": [\"metformin\", \"repaglinide\", \"nateglinide\", \"chlorpropamide\", \"glimepiride\",\n",
    "                          \"acetohexamide\", \"glipizide\", \"glyburide\", \"tolbutamide\", \"pioglitazone\",\n",
    "                          \"rosiglitazone\", \"acarbose\", \"miglitol\", \"troglitazone\", \"tolazamide\",\n",
    "                          \"examide\", \"citoglipton\", \"insulin\", \"glyburide-metformin\", \"glipizide-metformin\",\n",
    "                          \"glimepiride-pioglitazone\", \"metformin-rosiglitazone\", \"metformin-pioglitazone\"],\n",
    "            \"hospital_col\": None,                                # (dataset has no explicit hospital ID column)\n",
    "            \"specialty_col\": \"medical_specialty\",                # physician specialty\n",
    "            \"admission_type_col\": \"admission_type_id\",\n",
    "            \"discharge_disposition_col\": \"discharge_disposition_id\",\n",
    "            \"admission_source_col\": \"admission_source_id\"\n",
    "        },\n",
    "        \"preprocessing\": {\n",
    "            \"numeric_imputer\": \"mean\",           # impute missing numeric with mean\n",
    "            \"categorical_imputer\": \"most_frequent\",  # impute missing categoricals with mode\n",
    "            \"unknown_label\": \"UNKNOWN\",          # label for unseen or rare categories\n",
    "            \"use_unknown_category\": True,        # add an \"UNKNOWN\" category for unseen values\n",
    "            \"min_freq_for_category\": 5,          # rare category threshold (below this -> UNKNOWN)\n",
    "            \"truncate_icd_to_3_digits\": True     # use only first 3 digits of ICD codes to group\n",
    "        },\n",
    "        \"splits\": {\n",
    "            \"group_by\": \"patient\",    # group splits by patient_id to avoid leakage:contentReference[oaicite:6]{index=6}\n",
    "            \"n_splits\": 5,           # use 5-fold split (first fold for train/val, second for test)\n",
    "            \"stratify_by_target\": True,\n",
    "            \"seed\": 42\n",
    "        }\n",
    "    },\n",
    "    \"graph\": {\n",
    "        # Enable various node and edge types in the heterogeneous graph\n",
    "        \"node_types_enabled\": {\n",
    "            \"encounter\": True, \"icd\": True, \"icd_group\": True, \"drug\": True, \"drug_class\": True,\n",
    "            \"specialty\": True, \"admission_type\": True, \"discharge_disposition\": True, \"admission_source\": True,\n",
    "            \"hosp\": False  # no hospital node since not in dataset\n",
    "        },\n",
    "        \"edge_types_enabled\": {\n",
    "            \"encounter__has_icd__icd\": True,\n",
    "            \"icd__is_a__icd_group\": True,\n",
    "            \"encounter__has_drug__drug\": True,\n",
    "            \"drug__belongs_to__drug_class\": True,\n",
    "            \"encounter__has_specialty__specialty\": True,\n",
    "            \"encounter__has_admission_type__admission_type\": True,\n",
    "            \"encounter__has_discharge__discharge_disposition\": True,\n",
    "            \"encounter__has_admission_source__admission_source\": True,\n",
    "            \"reverse_edges\": True    # add reverse of every relation for undirected information flow\n",
    "        },\n",
    "        \"edge_featuring\": {\n",
    "            \"has_drug\": {\n",
    "                \"relation_subtypes_by_status\": True,  # separate edge types for Up/Down/Steady drug status\n",
    "                \"edge_attr_status\": True              # include an edge attribute indicating drug change\n",
    "            }\n",
    "        },\n",
    "        \"fanouts\": {\n",
    "            # Neighbor sampling fanout per edge type per GNN layer (2-layer example):\n",
    "            \"encounter__has_icd__icd\": [10, 5],\n",
    "            \"encounter__has_drug__drug\": [5, 5],\n",
    "            \"encounter__has_specialty__specialty\": [-1],  # -1 means take all neighbors (specialty has 1 neighbor per encounter)\n",
    "            \"encounter__has_admission_type__admission_type\": [-1],\n",
    "            \"encounter__has_discharge__discharge_disposition\": [-1],\n",
    "            \"encounter__has_admission_source__admission_source\": [-1],\n",
    "            \"icd__is_a__icd_group\": [-1],\n",
    "            \"drug__belongs_to__drug_class\": [-1],\n",
    "            \"reverse_edges\": [5, 5]  # sample some reverse edges if needed\n",
    "        }\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"arch\": \"HGT\",           # model architecture: \"HGT\", \"RGCN\", or \"GraphSAGE\"\n",
    "        \"hidden_dim\": 64,        # hidden embedding size\n",
    "        \"num_layers\": 2,         # number of GNN layers\n",
    "        \"heads\": 2,              # number of attention heads (for HGT)\n",
    "        \"rgcn_bases\": 0       # number of bases for RGCN\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"epochs\": 30,\n",
    "        \"early_stopping_patience\": 5,\n",
    "        \"val_every\": 1,          # evaluate on val every epoch\n",
    "        \"gradient_clip_norm\": 2.0,\n",
    "        \"optimizer\": {\n",
    "            \"name\": \"Adam\",\n",
    "            \"lr\":  0.001,\n",
    "            \"weight_decay\": 0.05,\n",
    "        },\n",
    "        \"batching\": {\n",
    "            \"batch_size_encounters\": 128\n",
    "        }\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"output_predictions_path\": \"artifacts/predictions.csv\"\n",
    "    },\n",
    "    \"evaluation\":{\n",
    "        \"metrics_primary\": [],\n",
    "        \"metrics_secondary\": [],\n",
    "    },\n",
    "    \"baseline\": {},\n",
    "    \"path\": {\n",
    "        \"artifacts_dir\": \"artifacts/\",\n",
    "        \"tb_log_dir\": \"artifacts/tb_logs/\",\n",
    "        \"logging_path\": \"logs/\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize config object\n",
    "config = Config(**config_dict)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T22:38:26.786622Z",
     "start_time": "2025-10-22T22:38:25.573494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.system import apply_system_config  # or from the cell above\n",
    "rt = apply_system_config(config)\n",
    "device = rt[\"device\"]\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "set_seed(config, 42)\n",
    "\n",
    "# Initialize logging and TensorBoard writer\n",
    "logger, writer = init_logging(config.path.logs_dir)\n",
    "logger.info(\"Configuration and logging initialized.\")"
   ],
   "id": "e80cbab047694921",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 16:38:26 INFO:Configuration and logging initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numexpr threads: 4\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T22:38:28.452187Z",
     "start_time": "2025-10-22T22:38:27.328003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from data import loader\n",
    "\n",
    "# Load the datasets\n",
    "df = loader.load_csv(config.data.csv_path)\n",
    "df = loader.apply_filters(df, config)\n",
    "\n",
    "logger.info(f\"Raw data shape: {df.shape}\")\n",
    "logger.info(f\"Columns: {list(df.columns)}\")"
   ],
   "id": "c840997f9e2dcd29",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 16:38:28 INFO:Raw data shape: (69990, 50)\n",
      "2025-10-22 16:38:28 INFO:Columns: ['encounter_id', 'patient_nbr', 'race', 'gender', 'age', 'weight', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'time_in_hospital', 'payer_code', 'medical_specialty', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'diag_1', 'diag_2', 'diag_3', 'number_diagnoses', 'max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone', 'change', 'diabetesMed', 'readmitted']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T22:38:30.335128Z",
     "start_time": "2025-10-22T22:38:28.558790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data import preprocess\n",
    "\n",
    "# Create train/val/test splits first (to fit imputer/scaler on train only)\n",
    "from data.splits import create_splits\n",
    "splits = create_splits(df, config)\n",
    "train_idx, val_idx, test_idx = splits\n",
    "\n",
    "df_train = df.iloc[train_idx].copy().reset_index(drop=True)\n",
    "df_val = df.iloc[val_idx].copy().reset_index(drop=True)\n",
    "df_test = df.iloc[test_idx].copy().reset_index(drop=True) if test_idx is not None else None\n",
    "\n",
    "logger.info(f\"Split sizes -> Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test)}\")\n",
    "\n",
    "# Preprocess the splits\n",
    "df_train, df_val, df_test, scaler = preprocess.preprocess_data(df_train, df_val, df_test, config)\n",
    "\n",
    "# The scaler and any encodings from train are now ready for use in inference too\n",
    "logger.info(\"Preprocessing complete. Sample of processed features:\")\n",
    "logger.info(df_train[config.data.columns.numeric + config.data.columns.categorical_low_card].head(3))\n"
   ],
   "id": "9b3f9b1be2f95783",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 16:38:29 INFO:Split sizes -> Train: 55992, Val: 13998, Test: 13998\n",
      "2025-10-22 16:38:30 INFO:Preprocessing complete. Sample of processed features:\n",
      "2025-10-22 16:38:30 INFO:   time_in_hospital  num_lab_procedures  num_procedures  num_medications  \\\n",
      "0          2.970598            1.261342        0.329610         1.482870   \n",
      "1          2.629817           -0.497552        0.899788         0.278178   \n",
      "2          1.607474            0.206006        0.329610         0.157708   \n",
      "\n",
      "   number_outpatient  number_emergency  number_inpatient  number_diagnoses  \\\n",
      "0          -0.260879         -0.198924          -0.29252          0.386081   \n",
      "1          -0.260879         -0.198924          -0.29252          0.386081   \n",
      "2          -0.260879         -0.198924          -0.29252          0.886218   \n",
      "\n",
      "              race  gender       age max_glu_serum A1Cresult change  \\\n",
      "0        Caucasian  Female   [80-90)          Norm        >8     Ch   \n",
      "1        Caucasian  Female  [90-100)          Norm        >8     Ch   \n",
      "2  AfricanAmerican  Female   [40-50)          Norm        >8     No   \n",
      "\n",
      "  diabetesMed  \n",
      "0         Yes  \n",
      "1         Yes  \n",
      "2         Yes  \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T22:38:30.940771Z",
     "start_time": "2025-10-22T22:38:30.523383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data import vocab\n",
    "\n",
    "vocabs, mappings = vocab.make_vocabs(df_train, config)\n",
    "logger.info(\"Vocab sizes: \" + \", \".join(f\"{k}: {len(v)}\" for k,v in vocabs.items()))\n"
   ],
   "id": "a394f35fd8177c51",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 16:38:30 INFO:Vocab sizes: icd: 844, icd_group: 844, drug: 23, drug_class: 1, admission_type: 9, discharge_disposition: 22, admission_source: 18, specialty: 68\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T22:38:31.086201Z",
     "start_time": "2025-10-22T22:38:30.951297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verify no patient overlap between train, val, test\n",
    "train_patients = set(df_train[config.data.identifier_cols.patient_id])\n",
    "val_patients = set(df_val[config.data.identifier_cols.patient_id])\n",
    "test_patients = set(df_test[config.data.identifier_cols.patient_id])\n",
    "\n",
    "overlap_train_val = train_patients.intersection(val_patients)\n",
    "overlap_train_test = train_patients.intersection(test_patients)\n",
    "overlap_val_test = val_patients.intersection(test_patients)\n",
    "logger.info(f\"Patient overlap - Train/Val: {len(overlap_train_val)}, Train/Test: {len(overlap_train_test)}, Val/Test: {len(overlap_val_test)}\")\n",
    "\n",
    "# Ensure target stratification roughly preserved\n",
    "mean_train = df_train[config.data.target.binarized_name].mean(); mean_val = df_val[config.data.target.binarized_name].mean(); mean_test = df_test[config.data.target.binarized_name].mean()\n",
    "logger.info(f\"Readmit rate - Train: {mean_train:.3f}, Val: {mean_val:.3f}, Test: {mean_test:.3f}\")"
   ],
   "id": "fcfcc24c7167617f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 16:38:31 INFO:Patient overlap - Train/Val: 0, Train/Test: 13998, Val/Test: 0\n",
      "2025-10-22 16:38:31 INFO:Readmit rate - Train: 0.089, Val: 0.091, Test: 0.090\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T22:38:45.230259Z",
     "start_time": "2025-10-22T22:38:31.100214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from graph import builder\n",
    "\n",
    "# Build heterogeneous graphs for each split\n",
    "graph_train = builder.build_heterodata(df_train, vocabs, config, include_target=True)\n",
    "graph_val   = builder.build_heterodata(df_val, vocabs, config, include_target=True)\n",
    "graph_test  = builder.build_heterodata(df_test, vocabs, config, include_target=True)\n",
    "\n",
    "# Log graph statistics\n",
    "logger.info(f\"Graph (Train) node types: {list(graph_train.node_types)}\")\n",
    "for ntype in graph_train.node_types:\n",
    "    logger.info(f\"  {ntype}: {graph_train[ntype].num_nodes} nodes\")\n",
    "logger.info(f\"Graph (Train) edge types: {list(graph_train.edge_types)}\")\n",
    "for etype in graph_train.edge_types:\n",
    "    logger.info(f\"  {etype}: {graph_train[etype].edge_index.size(1)} edges\")\n"
   ],
   "id": "49cc7e334f395a30",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 16:38:45 INFO:Graph (Train) node types: ['encounter', 'icd', 'icd_group', 'drug', 'drug_class', 'admission_type', 'discharge_disposition', 'admission_source', 'specialty']\n",
      "2025-10-22 16:38:45 INFO:  encounter: 55992 nodes\n",
      "2025-10-22 16:38:45 INFO:  icd: 844 nodes\n",
      "2025-10-22 16:38:45 INFO:  icd_group: 844 nodes\n",
      "2025-10-22 16:38:45 INFO:  drug: 23 nodes\n",
      "2025-10-22 16:38:45 INFO:  drug_class: 1 nodes\n",
      "2025-10-22 16:38:45 INFO:  admission_type: 9 nodes\n",
      "2025-10-22 16:38:45 INFO:  discharge_disposition: 22 nodes\n",
      "2025-10-22 16:38:45 INFO:  admission_source: 18 nodes\n",
      "2025-10-22 16:38:45 INFO:  specialty: 68 nodes\n",
      "2025-10-22 16:38:45 INFO:Graph (Train) edge types: [('encounter', 'has_icd', 'icd'), ('icd', 'rev_has_icd', 'encounter'), ('icd', 'is_a', 'icd_group'), ('icd_group', 'rev_is_a', 'icd'), ('encounter', 'has_drug_steady', 'drug'), ('drug', 'rev_has_drug_steady', 'encounter'), ('drug', 'belongs_to', 'drug_class'), ('drug_class', 'rev_belongs_to', 'drug'), ('encounter', 'has_specialty', 'specialty'), ('specialty', 'rev_has_specialty', 'encounter'), ('encounter', 'has_admission_source', 'admission_source'), ('admission_source', 'rev_has_admission_source', 'encounter'), ('encounter', 'has_admission_type', 'admission_type'), ('admission_type', 'rev_has_admission_type', 'encounter'), ('encounter', 'has_discharge', 'discharge_disposition'), ('discharge_disposition', 'rev_has_discharge', 'encounter'), ('encounter', 'has_drug_up', 'drug'), ('drug', 'rev_has_drug_up', 'encounter'), ('encounter', 'has_drug_down', 'drug'), ('drug', 'rev_has_drug_down', 'encounter')]\n",
      "2025-10-22 16:38:45 INFO:  ('encounter', 'has_icd', 'icd'): 166738 edges\n",
      "2025-10-22 16:38:45 INFO:  ('icd', 'rev_has_icd', 'encounter'): 166738 edges\n",
      "2025-10-22 16:38:45 INFO:  ('icd', 'is_a', 'icd_group'): 166738 edges\n",
      "2025-10-22 16:38:45 INFO:  ('icd_group', 'rev_is_a', 'icd'): 166738 edges\n",
      "2025-10-22 16:38:45 INFO:  ('encounter', 'has_drug_steady', 'drug'): 51981 edges\n",
      "2025-10-22 16:38:45 INFO:  ('drug', 'rev_has_drug_steady', 'encounter'): 51981 edges\n",
      "2025-10-22 16:38:45 INFO:  ('drug', 'belongs_to', 'drug_class'): 66729 edges\n",
      "2025-10-22 16:38:45 INFO:  ('drug_class', 'rev_belongs_to', 'drug'): 66729 edges\n",
      "2025-10-22 16:38:45 INFO:  ('encounter', 'has_specialty', 'specialty'): 55992 edges\n",
      "2025-10-22 16:38:45 INFO:  ('specialty', 'rev_has_specialty', 'encounter'): 55992 edges\n",
      "2025-10-22 16:38:45 INFO:  ('encounter', 'has_admission_source', 'admission_source'): 55992 edges\n",
      "2025-10-22 16:38:45 INFO:  ('admission_source', 'rev_has_admission_source', 'encounter'): 55992 edges\n",
      "2025-10-22 16:38:45 INFO:  ('encounter', 'has_admission_type', 'admission_type'): 55992 edges\n",
      "2025-10-22 16:38:45 INFO:  ('admission_type', 'rev_has_admission_type', 'encounter'): 55992 edges\n",
      "2025-10-22 16:38:45 INFO:  ('encounter', 'has_discharge', 'discharge_disposition'): 55992 edges\n",
      "2025-10-22 16:38:45 INFO:  ('discharge_disposition', 'rev_has_discharge', 'encounter'): 55992 edges\n",
      "2025-10-22 16:38:45 INFO:  ('encounter', 'has_drug_up', 'drug'): 7587 edges\n",
      "2025-10-22 16:38:45 INFO:  ('drug', 'rev_has_drug_up', 'encounter'): 7587 edges\n",
      "2025-10-22 16:38:45 INFO:  ('encounter', 'has_drug_down', 'drug'): 7161 edges\n",
      "2025-10-22 16:38:45 INFO:  ('drug', 'rev_has_drug_down', 'encounter'): 7161 edges\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T22:38:46.249117Z",
     "start_time": "2025-10-22T22:38:45.837804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Jupyter cell (with tqdm bars)\n",
    "import torch\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm.auto import tqdm, trange  # pretty progress bars in notebooks/terminals\n",
    "\n",
    "from train.model_factory import get_model_class, setup_and_compile_model\n",
    "from train.optim import make_optimizer, make_scheduler\n",
    "from train.sampling import build_num_neighbors\n",
    "from train.losses import make_criterion\n",
    "from train.device import get_device\n",
    "# sizes from the TRAIN graph\n",
    "enc_input_dim = graph_train['encounter'].x.size(-1)\n",
    "type_vocab_sizes = {nt: graph_train[nt].num_nodes for nt in graph_train.node_types if nt != 'encounter'}\n",
    "\n",
    "# optional one-time sanity check\n",
    "def validate_indices(g):\n",
    "    for nt in g.node_types:\n",
    "        if nt == 'encounter':\n",
    "            continue\n",
    "        x = g[nt].x\n",
    "        if x.numel() == 0:\n",
    "            continue\n",
    "        vmax = int(x.max().item())\n",
    "        n = int(g[nt].num_nodes)\n",
    "        assert vmax < n, f\"[{nt}] max index {vmax} >= num_nodes {n}\"\n",
    "validate_indices(graph_train)\n",
    "\n",
    "# build model (move to device after full construction)\n",
    "device = get_device()\n",
    "ModelClass = get_model_class(config)\n",
    "metadata = (list(graph_train.node_types), list(graph_train.edge_types))\n",
    "model = ModelClass(\n",
    "    metadata,\n",
    "    config,\n",
    "    enc_input_dim=enc_input_dim,\n",
    "    type_vocab_sizes=type_vocab_sizes,\n",
    ").to(device)  # or pass device=device in the constructor instead\n",
    "model = setup_and_compile_model(model, config, logger)"
   ],
   "id": "13e786150c07ce18",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 16:38:46 INFO:Skipping torch.compile (Windows not supported). Set system.cuda.compile_mode=None to silence this permanently.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-22T22:39:48.902394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from train.loop import train_one_epoch\n",
    "from train.loader import make_neighbor_loader\n",
    "# ---------------------------\n",
    "# Optimizer, scheduler, loss\n",
    "# ---------------------------\n",
    "optimizer = make_optimizer(model.parameters(), config)\n",
    "scheduler = make_scheduler(optimizer, config)\n",
    "criterion = make_criterion(graph_train, config, device)\n",
    "\n",
    "# ---------------------------\n",
    "# NeighborLoader\n",
    "# ---------------------------\n",
    "num_layers = int(getattr(config.model, \"num_layers\", 2))\n",
    "num_neighbors = build_num_neighbors(graph_train, config, num_layers)\n",
    "\n",
    "batch_size = getattr(getattr(config.train, \"batching\", object()), \"batch_size_encounters\",\n",
    "                     getattr(config.train, \"batch_size\", 1024))\n",
    "train_loader = make_neighbor_loader(\n",
    "    graph_train,\n",
    "    input_nodes=(\"encounter\", torch.arange(graph_train[\"encounter\"].num_nodes)),\n",
    "    num_neighbors=num_neighbors,\n",
    "    config=config,\n",
    "    train=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_data = graph_val.to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# Train loop (early stopping on AUPRC) with tqdm bars\n",
    "# ---------------------------\n",
    "best_val_metric = float(\"-inf\")\n",
    "best_state = None\n",
    "patience_counter = 0\n",
    "val_every = int(getattr(config.train, \"val_every\", 1))\n",
    "grad_clip = getattr(config.train, \"gradient_clip_norm\", None)\n",
    "epochs = int(config.train.epochs)\n",
    "amp_enabled = rt[\"amp_enabled\"]\n",
    "amp_dtype   = rt[\"amp_dtype\"]\n",
    "scaler      = rt[\"scaler\"]\n",
    "\n",
    "# outer epoch bar\n",
    "epoch_bar = trange(1, epochs + 1, desc=\"Epochs\", dynamic_ncols=True, leave=True)\n",
    "\n",
    "\n",
    "for epoch in epoch_bar:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_nodes = graph_train[\"encounter\"].num_nodes\n",
    "\n",
    "    # inner batch bar\n",
    "    batch_bar = tqdm(train_loader,\n",
    "                     desc=f\"Train Epoch {epoch}\",\n",
    "                     dynamic_ncols=True,\n",
    "                     leave=False)\n",
    "\n",
    "    avg_loss = train_one_epoch(\n",
    "        model=model,\n",
    "        train_loader=train_loader,      # or pass a tqdm wrapper as batch_bar\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        config=config,\n",
    "        rt=rt,\n",
    "        batch_bar=batch_bar,            # optional: tqdm(train_loader)\n",
    "        grad_clip=config.train.gradient_clip_norm\n",
    "    )\n",
    "\n",
    "    # step scheduler once per epoch\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    if \"writer\" in globals():\n",
    "        writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    val_auprc = None\n",
    "    if epoch % val_every == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits = model(val_data.x_dict, val_data.edge_index_dict)\n",
    "            val_probs = torch.sigmoid(val_logits).detach().cpu().numpy()\n",
    "            val_labels = graph_val[\"encounter\"].y.detach().cpu().numpy()\n",
    "            val_auprc = average_precision_score(val_labels, val_probs)\n",
    "\n",
    "        if \"writer\" in globals():\n",
    "            writer.add_scalar(\"Val/AUPRC\", val_auprc, epoch)\n",
    "\n",
    "        # early stopping\n",
    "        if val_auprc > best_val_metric:\n",
    "            best_val_metric = val_auprc\n",
    "            best_state = {\"model\": model.state_dict(), \"epoch\": epoch, \"val_auprc\": val_auprc}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "    # update epoch bar postfix (show both metrics if available)\n",
    "    if val_auprc is not None:\n",
    "        epoch_bar.set_postfix(avg_loss=f\"{avg_loss:.4f}\", val_auprc=f\"{val_auprc:.4f}\")\n",
    "        logger.info(f\"Epoch {epoch} - Train loss: {avg_loss:.4f}  |  Val AUPRC: {val_auprc:.4f}\")\n",
    "    else:\n",
    "        epoch_bar.set_postfix(avg_loss=f\"{avg_loss:.4f}\")\n",
    "        logger.info(f\"Epoch {epoch} - Train loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if patience_counter >= int(config.train.early_stopping_patience):\n",
    "        tqdm.write(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# ---------------------------\n",
    "# Load best weights\n",
    "# ---------------------------\n",
    "if best_state:\n",
    "    model.load_state_dict(best_state[\"model\"])\n",
    "    logger.info(f\"Loaded best model from epoch {best_state['epoch']} with Val AUPRC={best_state['val_auprc']:.4f}\")\n"
   ],
   "id": "e10013b4069cfe6d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epochs:   0%|          | 0/30 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7765041ce43f447ab3745ddb6f33399d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Train Epoch 1:   0%|          | 0/438 [00:01<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b7b50eef3624bc5b41eaf91467c90ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 17:08:15 INFO:Epoch 1 - Train loss: 0.3917  |  Val AUPRC: 0.1190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train Epoch 2:   0%|          | 0/438 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43793e17f3a1409fbe379109e5d1d5bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 17:34:07 INFO:Epoch 2 - Train loss: 0.3190  |  Val AUPRC: 0.1201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train Epoch 3:   0%|          | 0/438 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c679f6fdef8d44099e3799833491f31a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from train.metrics import compute_metrics, find_best_threshold\n",
    "from train.calibration import calibrate_probabilities, apply_calibration\n",
    "from train.loader import make_neighbor_loader\n",
    "\n",
    "# Evaluate on validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_out = model(val_data.x_dict, val_data.edge_index_dict)\n",
    "    test_data = graph_test.to(model.device)\n",
    "    test_out = model(test_data.x_dict, test_data.edge_index_dict)\n",
    "val_probs = torch.sigmoid(val_out).cpu().numpy()\n",
    "test_probs = torch.sigmoid(test_out).cpu().numpy()\n",
    "val_labels = graph_val['encounter'].y.cpu().numpy()\n",
    "test_labels = graph_test['encounter'].y.cpu().numpy()\n",
    "\n",
    "# Compute metrics at default 0.5 threshold\n",
    "val_metrics = compute_metrics(val_labels, val_probs)\n",
    "test_metrics = compute_metrics(test_labels, test_probs)\n",
    "logger.info(\"Validation metrics at 0.5 threshold: \" + \", \".join(f\"{k}={v:.4f}\" for k,v in val_metrics.items()))\n",
    "logger.info(\"Test metrics at 0.5 threshold: \" + \", \".join(f\"{k}={v:.4f}\" for k,v in test_metrics.items()))\n",
    "\n",
    "# Find best threshold on validation for F1 score\n",
    "best_thr, best_f1 = find_best_threshold(val_labels, val_probs, optimize_for='f1_pos')\n",
    "logger.info(f\"Best threshold for F1 on val = {best_thr:.2f}, F1 at best thr = {best_f1:.4f}\")\n",
    "# Apply this threshold to test set\n",
    "test_pred_opt = (test_probs >= best_thr).astype(int)\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "f1_test_opt = f1_score(test_labels, test_pred_opt, pos_label=1)\n",
    "precision_test_opt = precision_score(test_labels, test_pred_opt, pos_label=1, zero_division=0)\n",
    "recall_test_opt = recall_score(test_labels, test_pred_opt, pos_label=1)\n",
    "logger.info(f\"Test F1={f1_test_opt:.4f}, Precision={precision_test_opt:.4f}, Recall={recall_test_opt:.4f} at threshold {best_thr:.2f}\")\n",
    "\n",
    "# Probability calibration (using validation set)\n",
    "cal_model = calibrate_probabilities(val_probs, val_labels, method=\"platt\")  # or \"isotonic\"\n",
    "cal_test_probs = apply_calibration(cal_model, test_probs)\n",
    "cal_metrics = compute_metrics(test_labels, cal_test_probs)\n",
    "logger.info(\"Test metrics after calibration: \" + \", \".join(f\"{k}={v:.4f}\" for k,v in cal_metrics.items()))\n"
   ],
   "id": "2d4647db3b7dd15a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay, CalibrationDisplay, ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "# ROC Curve\n",
    "RocCurveDisplay.from_predictions(test_labels, test_probs)\n",
    "plt.title(\"ROC Curve (Test)\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "PrecisionRecallDisplay.from_predictions(test_labels, test_probs)\n",
    "plt.title(\"Precision-Recall Curve (Test)\")\n",
    "plt.show()\n",
    "\n",
    "# Calibration curve (reliability diagram)\n",
    "CalibrationDisplay.from_predictions(test_labels, test_probs, n_bins=10, strategy='uniform')\n",
    "plt.title(\"Calibration Curve (Test)\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix at optimal threshold\n",
    "cm = confusion_matrix(test_labels, test_pred_opt)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Readmit\",\"Readmit<30\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(f\"Confusion Matrix (Thr={best_thr:.2f})\")\n",
    "plt.show()\n",
    "\n",
    "# Decision curve analysis (net benefit vs threshold)\n",
    "from src.train.threshold import decision_curve_analysis\n",
    "dc_df = decision_curve_analysis(test_labels, test_probs)\n",
    "plt.plot(dc_df['threshold'], dc_df['net_benefit'], label='GNN Model')\n",
    "plt.axhline(y=0, color='k', linestyle='--')\n",
    "plt.xlabel(\"Threshold Probability\")\n",
    "plt.ylabel(\"Net Benefit\")\n",
    "plt.title(\"Decision Curve (Test Set)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "45835d319ebf65c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from src.infer.batch_predict import batch_predict\n",
    "\n",
    "# Suppose we have a CSV \"raw/new_encounters.csv\" of new patient encounters to predict\n",
    "new_data_path = \"raw/new_encounters.csv\"\n",
    "if os.path.exists(new_data_path):\n",
    "    predictions_df = batch_predict(new_data_path, model, vocabs, config)\n",
    "    logger.info(f\"Inductive predictions saved to: {config.inference.output_predictions_path}\")\n",
    "    logger.info(predictions_df.head())\n"
   ],
   "id": "c2d5b17d9840af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from src.train.metrics import compute_metrics\n",
    "\n",
    "def train_and_eval_baselines(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Train multiple baseline ML models and evaluate on val and test sets.\"\"\"\n",
    "    results = {}\n",
    "    # Define models with some default hyperparameters\n",
    "    models = {\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "        \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "        \"SVM\": SVC(kernel='rbf', probability=True, random_state=42),\n",
    "        \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "    }\n",
    "    for name, model in models.items():\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        # Evaluate on validation set\n",
    "        if X_val is not None and y_val is not None:\n",
    "            # Some models (e.g. SVM without probability) we set probability=True for consistency\n",
    "            val_prob = model.predict_proba(X_val)[:, 1] if hasattr(model, \"predict_proba\") else model.decision_function(X_val)\n",
    "            val_metrics = compute_metrics(y_val, val_prob)\n",
    "        else:\n",
    "            val_metrics = {}\n",
    "        # Evaluate on test set\n",
    "        test_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else model.decision_function(X_test)\n",
    "        test_metrics = compute_metrics(y_test, test_prob)\n",
    "        results[name] = {\"val_metrics\": val_metrics, \"test_metrics\": test_metrics}\n",
    "    return results\n"
   ],
   "id": "fb8382dcce979cad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from benchmarks.baselines import train_and_eval_baselines\n",
    "\n",
    "# Prepare NumPy arrays for baseline models\n",
    "feature_cols = config.data.columns.numeric + config.data.columns.categorical_low_card\n",
    "X_train_tab = df_train_proc[feature_cols].to_numpy()\n",
    "y_train_tab = df_train_proc[target_col].to_numpy()\n",
    "X_val_tab = df_val_proc[feature_cols].to_numpy()\n",
    "y_val_tab = df_val_proc[target_col].to_numpy()\n",
    "X_test_tab = df_test_proc[feature_cols].to_numpy()\n",
    "y_test_tab = df_test_proc[target_col].to_numpy()\n",
    "\n",
    "# Train and evaluate baseline models\n",
    "baseline_results = train_and_eval_baselines(X_train_tab, y_train_tab, X_val_tab, y_val_tab, X_test_tab, y_test_tab)\n",
    "\n",
    "# Display baseline evaluation results\n",
    "for model_name, metrics_dict in baseline_results.items():\n",
    "    test_met = metrics_dict[\"test_metrics\"]\n",
    "    logger.info(f\"{model_name} - Test AUROC: {test_met['auroc']:.4f}, AUPRC: {test_met['auprc']:.4f}, \"\n",
    "                f\"F1: {test_met['f1_pos']:.4f}, Precision: {test_met['precision_pos']:.4f}, Recall: {test_met['recall_pos']:.4f}, \"\n",
    "                f\"Bal Acc: {test_met['balanced_accuracy']:.4f}\")\n"
   ],
   "id": "d6c51b8af4531475"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T06:32:12.013498Z",
     "start_time": "2025-10-22T06:32:11.609933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(\"torch:\", torch.__version__)      # e.g. 2.2.1\n",
    "print(\"cuda :\", torch.version.cuda)      # None==CPU, else like '12.1'\n"
   ],
   "id": "4bd5cfe55ce5a834",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.2.1\n",
      "cuda : 12.1\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d575aa37fbf972ed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
